{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "kogpt2 finetuning",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95R4RqiOuA7A",
        "outputId": "785f815a-7229-4192-87e7-2c1fda6a3ab1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPruP1vbulVa"
      },
      "source": [
        "# 필요한 필수 새팅 작업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHALfG-nWlRV",
        "outputId": "77395e00-2a87-47af-8c5f-e6c470af1ff4"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m6L6j_nYTTl",
        "outputId": "428f9dcf-3719-400c-9954-b123a0eeca8f"
      },
      "source": [
        "!pip install -r \"/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet==1.6.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: gluonnlp==0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 3)) (0.8.3)\n",
            "Requirement already satisfied: sentencepiece==0.1.6 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 4)) (0.1.6)\n",
            "Requirement already satisfied: torch==1.7.0+cu101 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 5)) (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision==0.8.1+cu101 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 6)) (0.8.1+cu101)\n",
            "Requirement already satisfied: transformers==2.1.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 7)) (2.1.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 8)) (2.1)\n",
            "Requirement already satisfied: dropbox in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 9)) (11.0.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.6.0->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 1)) (0.8.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.6.0->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.6.0->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 5)) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 5)) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.1+cu101->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 6)) (7.0.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 7)) (0.0.43)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 7)) (1.16.35)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 7)) (2019.12.20)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 8)) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: stone>=2.* in /usr/local/lib/python3.6/dist-packages (from dropbox->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 9)) (3.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 7)) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.35 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 7)) (1.19.35)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 7)) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 7)) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 8)) (50.3.2)\n",
            "Requirement already satisfied: ply>=3.4 in /usr/local/lib/python3.6/dist-packages (from stone>=2.*->dropbox->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 9)) (3.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.35->boto3->transformers==2.1.1->-r /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/requirements.txt (line 7)) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8M3DCwcYlMv"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master')\n",
        "logs_base_dir = \"runs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Jjj58pd1Rq"
      },
      "source": [
        "ctx= 'cuda'\n",
        "cachedir='~/kogpt2/'\n",
        "load_path = '/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/finetuning/checkpoint/' # 이어서 학습시킬 모델 경로\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/finetuning/checkpoint/' # 학습한 모델을 저장시킬 경로\n",
        "data_file_path = '/content/drive/MyDrive/Colab Notebooks/datasets/msg.csv' # 학습할 데이터셋 경로"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umcGNCCYktXo"
      },
      "source": [
        "# 모델 학습 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhZHXOMtlKCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5da8208-92fe-4997-8aab-eb64f5e5b214"
      },
      "source": [
        "from jupyter_main import main \r\n",
        "main(load_path = load_path, data_file_path = data_file_path, \\\r\n",
        "     save_path = '/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/checkpoint/', \\\r\n",
        "     summary_url = '/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/runs/2020-12-14/', \\\r\n",
        "     text_size = 1000, new = 0, batch_size = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n",
            "count 0 :  /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/finetuning/checkpoint/\n",
            "using cached model\n",
            "tokenizer ending\n",
            "(58, 3)\n",
            "KoGPT-2 Transfer Learning Start\n",
            "epoch no.0 train no.0  loss = 5258.50977 avg_loss = 5.25851\n",
            "epoch no.0 train no.10  loss = 4741.30615 avg_loss = 4.90803\n",
            "epoch no.0 train no.20  loss = 4788.68018 avg_loss = 4.76217\n",
            "epoch no.0 train no.30  loss = 4420.39600 avg_loss = 4.63993\n",
            "epoch no.0 train no.40  loss = 4599.13770 avg_loss = 4.55755\n",
            "epoch no.0 train no.50  loss = 4613.43506 avg_loss = 4.55635\n",
            "epoch no.1 train no.60  loss = 4384.77246 avg_loss = 4.52617\n",
            "epoch no.1 train no.70  loss = 4144.29443 avg_loss = 4.40557\n",
            "epoch no.1 train no.80  loss = 4161.22168 avg_loss = 4.33808\n",
            "epoch no.1 train no.90  loss = 3775.23145 avg_loss = 4.28513\n",
            "epoch no.1 train no.100  loss = 4228.41602 avg_loss = 4.25002\n",
            "epoch no.1 train no.110  loss = 4891.80908 avg_loss = 4.25247\n",
            "epoch no.2 train no.120  loss = 3619.21240 avg_loss = 4.12243\n",
            "epoch no.2 train no.130  loss = 4252.72266 avg_loss = 4.06737\n",
            "epoch no.2 train no.140  loss = 3959.31274 avg_loss = 4.03087\n",
            "epoch no.2 train no.150  loss = 4647.29834 avg_loss = 4.02625\n",
            "epoch no.2 train no.160  loss = 3161.76318 avg_loss = 3.96753\n",
            "epoch no.2 train no.170  loss = 3956.42554 avg_loss = 3.88433\n",
            "epoch no.3 train no.180  loss = 3494.92969 avg_loss = 3.81912\n",
            "epoch no.3 train no.190  loss = 3361.92847 avg_loss = 3.77484\n",
            "epoch no.3 train no.200  loss = 2568.04932 avg_loss = 3.71208\n",
            "epoch no.3 train no.210  loss = 3835.29224 avg_loss = 3.66746\n",
            "epoch no.3 train no.220  loss = 1994.85547 avg_loss = 3.61132\n",
            "epoch no.3 train no.230  loss = 3590.61987 avg_loss = 3.58037\n",
            "epoch no.4 train no.240  loss = 2625.31104 avg_loss = 3.51396\n",
            "epoch no.4 train no.250  loss = 1988.36658 avg_loss = 3.44524\n",
            "epoch no.4 train no.260  loss = 3042.22974 avg_loss = 3.41378\n",
            "epoch no.4 train no.270  loss = 3776.18677 avg_loss = 3.36849\n",
            "epoch no.4 train no.280  loss = 1482.51599 avg_loss = 3.32811\n",
            "epoch no.5 train no.290  loss = 1471.25317 avg_loss = 3.27257\n",
            "epoch no.5 train no.300  loss = 3067.42310 avg_loss = 3.25292\n",
            "epoch no.5 train no.310  loss = 3412.41187 avg_loss = 3.21011\n",
            "epoch no.5 train no.320  loss = 2846.52905 avg_loss = 3.13771\n",
            "epoch no.5 train no.330  loss = 3188.94971 avg_loss = 3.05700\n",
            "epoch no.5 train no.340  loss = 3641.20459 avg_loss = 3.06108\n",
            "epoch no.6 train no.350  loss = 2897.77539 avg_loss = 3.02798\n",
            "epoch no.6 train no.360  loss = 1991.33801 avg_loss = 2.99198\n",
            "epoch no.6 train no.370  loss = 3081.86938 avg_loss = 2.93381\n",
            "epoch no.6 train no.380  loss = 883.70428 avg_loss = 2.85687\n",
            "epoch no.6 train no.390  loss = 1541.44409 avg_loss = 2.85286\n",
            "epoch no.6 train no.400  loss = 1426.18628 avg_loss = 2.81380\n",
            "epoch no.7 train no.410  loss = 1175.55139 avg_loss = 2.74612\n",
            "epoch no.7 train no.420  loss = 2445.21216 avg_loss = 2.75136\n",
            "epoch no.7 train no.430  loss = 1670.85144 avg_loss = 2.73629\n",
            "epoch no.7 train no.440  loss = 1294.50757 avg_loss = 2.67437\n",
            "epoch no.7 train no.450  loss = 2645.32983 avg_loss = 2.61608\n",
            "epoch no.7 train no.460  loss = 1955.83362 avg_loss = 2.56024\n",
            "epoch no.8 train no.470  loss = 721.47150 avg_loss = 2.49543\n",
            "epoch no.8 train no.480  loss = 1343.72034 avg_loss = 2.45093\n",
            "epoch no.8 train no.490  loss = 538.12494 avg_loss = 2.43971\n",
            "epoch no.8 train no.500  loss = 2703.45752 avg_loss = 2.44149\n",
            "epoch no.8 train no.510  loss = 1011.82233 avg_loss = 2.41029\n",
            "epoch no.8 train no.520  loss = 852.48444 avg_loss = 2.33557\n",
            "epoch no.9 train no.530  loss = 1268.68530 avg_loss = 2.26764\n",
            "epoch no.9 train no.540  loss = 2628.30908 avg_loss = 2.24653\n",
            "epoch no.9 train no.550  loss = 2453.53809 avg_loss = 2.21487\n",
            "epoch no.9 train no.560  loss = 2133.52856 avg_loss = 2.17221\n",
            "epoch no.9 train no.570  loss = 928.06024 avg_loss = 2.13891\n",
            "epoch no.10 train no.580  loss = 2426.62109 avg_loss = 2.14686\n",
            "epoch no.10 train no.590  loss = 2372.30347 avg_loss = 2.11287\n",
            "epoch no.10 train no.600  loss = 2436.41406 avg_loss = 2.09993\n",
            "epoch no.10 train no.610  loss = 1004.14374 avg_loss = 2.06220\n",
            "epoch no.10 train no.620  loss = 2985.92407 avg_loss = 2.04088\n",
            "epoch no.10 train no.630  loss = 564.97638 avg_loss = 2.01340\n",
            "epoch no.11 train no.640  loss = 444.02304 avg_loss = 1.96007\n",
            "epoch no.11 train no.650  loss = 678.76221 avg_loss = 1.89779\n",
            "epoch no.11 train no.660  loss = 1182.61609 avg_loss = 1.87672\n",
            "epoch no.11 train no.670  loss = 1600.77051 avg_loss = 1.85949\n",
            "epoch no.11 train no.680  loss = 1976.92468 avg_loss = 1.83656\n",
            "epoch no.11 train no.690  loss = 3494.11890 avg_loss = 1.83559\n",
            "epoch no.12 train no.700  loss = 1420.84729 avg_loss = 1.81725\n",
            "epoch no.12 train no.710  loss = 2215.55811 avg_loss = 1.81903\n",
            "epoch no.12 train no.720  loss = 342.38486 avg_loss = 1.75384\n",
            "epoch no.12 train no.730  loss = 2240.90698 avg_loss = 1.72764\n",
            "epoch no.12 train no.740  loss = 2151.78613 avg_loss = 1.69483\n",
            "epoch no.12 train no.750  loss = 2504.38477 avg_loss = 1.70079\n",
            "epoch no.13 train no.760  loss = 3212.45044 avg_loss = 1.66055\n",
            "epoch no.13 train no.770  loss = 1307.26270 avg_loss = 1.66349\n",
            "epoch no.13 train no.780  loss = 774.60956 avg_loss = 1.66420\n",
            "epoch no.13 train no.790  loss = 1759.38025 avg_loss = 1.63978\n",
            "epoch no.13 train no.800  loss = 2441.02319 avg_loss = 1.60150\n",
            "epoch no.13 train no.810  loss = 1596.93762 avg_loss = 1.56544\n",
            "epoch no.14 train no.820  loss = 418.16754 avg_loss = 1.52152\n",
            "epoch no.14 train no.830  loss = 694.99304 avg_loss = 1.47161\n",
            "epoch no.14 train no.840  loss = 2458.14062 avg_loss = 1.46900\n",
            "epoch no.14 train no.850  loss = 2548.21362 avg_loss = 1.48505\n",
            "epoch no.14 train no.860  loss = 175.00015 avg_loss = 1.46665\n",
            "epoch no.15 train no.870  loss = 2057.51123 avg_loss = 1.45616\n",
            "epoch no.15 train no.880  loss = 692.40332 avg_loss = 1.44701\n",
            "epoch no.15 train no.890  loss = 638.70435 avg_loss = 1.41267\n",
            "epoch no.15 train no.900  loss = 1399.41602 avg_loss = 1.40623\n",
            "epoch no.15 train no.910  loss = 1271.36157 avg_loss = 1.40037\n",
            "epoch no.15 train no.920  loss = 168.98404 avg_loss = 1.34027\n",
            "epoch no.16 train no.930  loss = 1988.57507 avg_loss = 1.35637\n",
            "epoch no.16 train no.940  loss = 1893.89490 avg_loss = 1.31845\n",
            "epoch no.16 train no.950  loss = 2356.79248 avg_loss = 1.29907\n",
            "epoch no.16 train no.960  loss = 2333.56494 avg_loss = 1.33117\n",
            "epoch no.16 train no.970  loss = 291.05231 avg_loss = 1.31022\n",
            "epoch no.16 train no.980  loss = 1553.24805 avg_loss = 1.27262\n",
            "epoch no.17 train no.990  loss = 1311.58899 avg_loss = 1.27338\n",
            "epoch no.17 train no.1000  loss = 1802.23816 avg_loss = 1.24661\n",
            "epoch no.17 train no.1010  loss = 2353.32495 avg_loss = 1.22212\n",
            "epoch no.17 train no.1020  loss = 159.14583 avg_loss = 1.14870\n",
            "epoch no.17 train no.1030  loss = 1257.62195 avg_loss = 1.14220\n",
            "epoch no.17 train no.1040  loss = 1787.67188 avg_loss = 1.16062\n",
            "epoch no.18 train no.1050  loss = 216.61777 avg_loss = 1.13170\n",
            "epoch no.18 train no.1060  loss = 536.29504 avg_loss = 1.13750\n",
            "epoch no.18 train no.1070  loss = 178.36084 avg_loss = 1.08616\n",
            "epoch no.18 train no.1080  loss = 2061.53247 avg_loss = 1.07150\n",
            "epoch no.18 train no.1090  loss = 204.08490 avg_loss = 1.10115\n",
            "epoch no.18 train no.1100  loss = 498.21497 avg_loss = 1.09554\n",
            "epoch no.19 train no.1110  loss = 2108.07251 avg_loss = 1.09278\n",
            "epoch no.19 train no.1120  loss = 956.27209 avg_loss = 1.06344\n",
            "epoch no.19 train no.1130  loss = 2174.22119 avg_loss = 1.02693\n",
            "epoch no.19 train no.1140  loss = 2924.15161 avg_loss = 1.05170\n",
            "epoch no.19 train no.1150  loss = 464.03879 avg_loss = 1.04247\n",
            "epoch no.20 train no.1160  loss = 595.95197 avg_loss = 1.02035\n",
            "학습완료\n",
            "모델저장\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5vmsyryk_ct"
      },
      "source": [
        "# 모델 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0wCTMZcxkV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4b8a02-33e9-46b8-aa97-768d90f16bef"
      },
      "source": [
        "from jupyter_generator import main\r\n",
        "import time\r\n",
        "start = time.time()\r\n",
        "main(temperature=0.9, tmp_sent=\"새해에는\", text_size=100,loops=5,\\\r\n",
        "    load_path = '/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/checkpoint/msg/KoGPT2_checkpoint_1195.tar',\\\r\n",
        "    samples = \"/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/samples/\")\r\n",
        "print(\"Running Time : \", time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n",
            "using cached model\n",
            "weight load -  msg\n",
            "11\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '▁건강', '하시고', '▁뜻', '▁하시는', '▁일', '▁모두', '▁이루', '시기', '▁바랍니다', '.', '</s>']\n",
            "새해에는 더욱 건강하시고 뜻 하시는 일 모두 이루시기 바랍니다.\n",
            "13\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '▁많이', '▁받으', '시고', '하시는', '▁일', '▁모두', '▁잘', '되', '시', '길', '▁바랍니다', '합니다', '.']\n",
            "새해에는 복 많이 받으시고하시는 일 모두 잘되시길 기원합니다\n",
            "23\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '▁건강', '하시고', '▁뜻', '▁하시는', '▁일', '▁모두', '▁이루', '시기', '▁바랍니다', '.', '</s>', 'ky', 'Li', '▁방송', '▁30', '▁회', '▁차고', '▁감사', '드립니다', '.', '▁감사합니다', '.', '</s>']\n",
            "새해에는 더욱 건강하시고 뜻 하시는 일 모두 이루시기 바랍니다. skylife 제 32의 성원에 감사드립니다. 감사합니다.\n",
            "101\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '▁건강', '▁열심히', '▁하겠습니다', '.', '▁감사합니다', '.', '▁보낸', '사람', '▁:', '▁김주', '륜', '▁사원', '[', 'h', 'al', 'o', '7', ']', '▁받은', '시간', '▁:', '▁2020', '.11', '.02', '▁14', ':', '15', ':', '41', '▁[', '신규', '입', '자', '▁W', 'E', 'HA', 'GO', '▁회원가입', '▁요청', '의', '▁건', ']', '▁수신', '▁:', '▁더', '존', 'ICT', '그룹', '▁8', '월', '▁신규', '▁입사', '자', '▁참조', '▁:', '▁해당', '▁부서', '▁책임자', '▁안녕하세요', ',', '▁가상화', '운영', '팀', '▁김광', '륜', '입니다', '.', '▁인사', '팀에', '▁제출', '해주', '신', '▁외부', '▁메일', '▁주', '소로', '▁W', 'E', 'HA', 'GO', '▁초대', '▁메일', '▁발송', '했습니다', '.', '▁회원가입', '▁후', '▁쪽', '지', '▁회', '신', '▁주시', '면', '▁서비스', '▁및', '▁프로그램', '▁배포', '▁진행', '하겠습니다', '.', '▁감사합니다']\n",
            "새해에는 더욱 더 열심히 하겠습니다. 감사합니다. 보낸사람 : 김광륜 사원[halo7] 받은시간 : 2020.11.02 16:35:41 [신규 입사자 WEHAGO 회원가입 요청의 건] 수신 : 더존ICT그룹 8월 신규 입사자 참조 : 해당 부서 책임자 안녕하세요, 가상화운영팀 김광륜입니다. 인사팀에 제출해주신 외부 메일 주소로 WEHAGO 초대 메일 발송했습니다. 회원가입 후 쪽지 회신 주시면 서비스 및 프로그램 배포 진행하겠습니다. 감사합니다\n",
            "good\n",
            "Running Time :  16.83525252342224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H34JyXsNARjm",
        "outputId": "890c49c4-518e-48cf-fc04-eae6135d6de6"
      },
      "source": [
        "from loaded_model_generator import main\r\n",
        "main(temperature=0.9, text_size=1,loops=3,\\\r\n",
        "    load_path = '/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/checkpoint/msg/KoGPT2_checkpoint_1195.tar',\\\r\n",
        "    samples = \"/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/samples/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n",
            "using cached model\n",
            "ok :  msg\n",
            "새해에는\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '하는', '▁일들이']\n",
            "새해에는 소망하는 일들이\n",
            "Running Time :  0.21010494232177734\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '하는', '▁일들이']\n",
            "새해에는 소망하는 일들이\n",
            "Running Time :  0.07026410102844238\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '▁건강', '하시고']\n",
            "새해에는 더욱 건강하시고\n",
            "Running Time :  0.0699002742767334\n",
            "새해에는\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '▁건강', '하시고']\n",
            "새해에는 더욱 건강하시고\n",
            "Running Time :  0.07993817329406738\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '▁건강', '하시고']\n",
            "새해에는 더욱 건강하시고\n",
            "Running Time :  0.07620906829833984\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '존', 'ICT']\n",
            "새해에는 더존ICT\n",
            "Running Time :  0.07582640647888184\n",
            "새해에는\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '존', 'ICT']\n",
            "새해에는 더존ICT\n",
            "Running Time :  0.08458471298217773\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '▁건강', '▁열심히']\n",
            "새해에는 더욱 더 열심히\n",
            "Running Time :  0.07192301750183105\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁더욱', '존', 'ICT']\n",
            "새해에는 더존ICT\n",
            "Running Time :  0.06946349143981934\n",
            "새해\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁많이', '▁받으']\n",
            "새해 복 많이 받으\n",
            "Running Time :  0.0758810043334961\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁많이', '▁받으']\n",
            "새해 복 많이 받으\n",
            "Running Time :  0.07328248023986816\n",
            "2\n",
            "to_tokens: ['▁[', '▁복', '▁많이', '▁받으']\n",
            "새해 복 많이 받으\n",
            "Running Time :  0.07438397407531738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-26a5347b9582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mloaded_model_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/checkpoint/msg/KoGPT2_checkpoint_1195.tar'\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/samples/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-FineTuning-master/loaded_model_generator.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(temperature, top_p, top_k, tmp_sent, text_size, loops, load_path, ctx, cachedir, samples)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ok : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mmake_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONvFkFuS2_5Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}